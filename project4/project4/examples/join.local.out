
Lmod is automatically replacing "gcc/10.2.0/i62tgso" with "gcc/7.5.0".


Inactive Modules:
  1) openjdk/11.0.12_7/xkfgsx7

WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/cm/shared/apps/spack/cpu/opt/spack/linux-centos8-zen/gcc-8.3.1/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
24/10/11 11:09:18 INFO SparkContext: Running Spark version 3.2.1
24/10/11 11:09:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 11:09:18 INFO ResourceUtils: ==============================================================
24/10/11 11:09:18 INFO ResourceUtils: No custom resources configured for spark.driver.
24/10/11 11:09:18 INFO ResourceUtils: ==============================================================
24/10/11 11:09:18 INFO SparkContext: Submitted application: Join
24/10/11 11:09:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/10/11 11:09:18 INFO ResourceProfile: Limiting resource is cpu
24/10/11 11:09:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/10/11 11:09:18 INFO SecurityManager: Changing view acls to: ggupta1
24/10/11 11:09:18 INFO SecurityManager: Changing modify acls to: ggupta1
24/10/11 11:09:18 INFO SecurityManager: Changing view acls groups to: 
24/10/11 11:09:18 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 11:09:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ggupta1); groups with view permissions: Set(); users  with modify permissions: Set(ggupta1); groups with modify permissions: Set()
24/10/11 11:09:18 INFO Utils: Successfully started service 'sparkDriver' on port 44375.
24/10/11 11:09:18 INFO SparkEnv: Registering MapOutputTracker
24/10/11 11:09:18 INFO SparkEnv: Registering BlockManagerMaster
24/10/11 11:09:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/10/11 11:09:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/10/11 11:09:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/10/11 11:09:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-20bb64be-6227-4585-bc32-3d84be74ac0c
24/10/11 11:09:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/10/11 11:09:19 INFO SparkEnv: Registering OutputCommitCoordinator
24/10/11 11:09:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/10/11 11:09:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://exp-1-31.expanse.sdsc.edu:4040
24/10/11 11:09:19 INFO SparkContext: Added JAR file:/home/ggupta1/project4/examples/join.jar at spark://exp-1-31.expanse.sdsc.edu:44375/jars/join.jar with timestamp 1728670158166
24/10/11 11:09:19 INFO Executor: Starting executor ID driver on host exp-1-31.expanse.sdsc.edu
24/10/11 11:09:19 INFO Executor: Fetching spark://exp-1-31.expanse.sdsc.edu:44375/jars/join.jar with timestamp 1728670158166
24/10/11 11:09:19 INFO TransportClientFactory: Successfully created connection to exp-1-31.expanse.sdsc.edu/198.202.103.224:44375 after 31 ms (0 ms spent in bootstraps)
24/10/11 11:09:19 INFO Utils: Fetching spark://exp-1-31.expanse.sdsc.edu:44375/jars/join.jar to /tmp/spark-c352b939-5f18-4802-8e41-62daaa46bbfb/userFiles-01188ca8-df02-4381-9088-1b31c9f7d062/fetchFileTemp15235984728604058105.tmp
24/10/11 11:09:19 INFO Executor: Adding file:/tmp/spark-c352b939-5f18-4802-8e41-62daaa46bbfb/userFiles-01188ca8-df02-4381-9088-1b31c9f7d062/join.jar to class loader
24/10/11 11:09:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44735.
24/10/11 11:09:19 INFO NettyBlockTransferService: Server created on exp-1-31.expanse.sdsc.edu:44735
24/10/11 11:09:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/10/11 11:09:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, exp-1-31.expanse.sdsc.edu, 44735, None)
24/10/11 11:09:19 INFO BlockManagerMasterEndpoint: Registering block manager exp-1-31.expanse.sdsc.edu:44735 with 434.4 MiB RAM, BlockManagerId(driver, exp-1-31.expanse.sdsc.edu, 44735, None)
24/10/11 11:09:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, exp-1-31.expanse.sdsc.edu, 44735, None)
24/10/11 11:09:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, exp-1-31.expanse.sdsc.edu, 44735, None)
24/10/11 11:09:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 219.7 KiB, free 434.2 MiB)
24/10/11 11:09:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.1 KiB, free 434.2 MiB)
24/10/11 11:09:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on exp-1-31.expanse.sdsc.edu:44735 (size: 32.1 KiB, free: 434.4 MiB)
24/10/11 11:09:20 INFO SparkContext: Created broadcast 0 from textFile at JoinSpark.scala:8
24/10/11 11:09:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 219.7 KiB, free 433.9 MiB)
24/10/11 11:09:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.1 KiB, free 433.9 MiB)
24/10/11 11:09:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on exp-1-31.expanse.sdsc.edu:44735 (size: 32.1 KiB, free: 434.3 MiB)
24/10/11 11:09:20 INFO SparkContext: Created broadcast 1 from textFile at JoinSpark.scala:10
24/10/11 11:09:20 INFO FileInputFormat: Total input files to process : 1
24/10/11 11:09:20 INFO FileInputFormat: Total input files to process : 1
24/10/11 11:09:20 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
24/10/11 11:09:20 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
24/10/11 11:09:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
24/10/11 11:09:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
24/10/11 11:09:21 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
24/10/11 11:09:21 INFO DAGScheduler: Registering RDD 6 (map at JoinSpark.scala:12) as input to shuffle 0
24/10/11 11:09:21 INFO DAGScheduler: Registering RDD 7 (map at JoinSpark.scala:12) as input to shuffle 1
24/10/11 11:09:21 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 3 output partitions
24/10/11 11:09:21 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)
24/10/11 11:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
24/10/11 11:09:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
24/10/11 11:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at map at JoinSpark.scala:12), which has no missing parents
24/10/11 11:09:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.0 KiB, free 433.9 MiB)
24/10/11 11:09:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.9 MiB)
24/10/11 11:09:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on exp-1-31.expanse.sdsc.edu:44735 (size: 3.5 KiB, free: 434.3 MiB)
24/10/11 11:09:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
24/10/11 11:09:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at map at JoinSpark.scala:12) (first 15 tasks are for partitions Vector(0, 1))
24/10/11 11:09:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
24/10/11 11:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at map at JoinSpark.scala:12), which has no missing parents
24/10/11 11:09:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.0 KiB, free 433.9 MiB)
24/10/11 11:09:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.9 MiB)
24/10/11 11:09:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on exp-1-31.expanse.sdsc.edu:44735 (size: 3.4 KiB, free: 434.3 MiB)
24/10/11 11:09:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
24/10/11 11:09:21 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at map at JoinSpark.scala:12) (first 15 tasks are for partitions Vector(0, 1, 2))
24/10/11 11:09:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0
24/10/11 11:09:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (exp-1-31.expanse.sdsc.edu, executor driver, partition 0, PROCESS_LOCAL, 4500 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (exp-1-31.expanse.sdsc.edu, executor driver, partition 1, PROCESS_LOCAL, 4500 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/10/11 11:09:21 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
24/10/11 11:09:21 INFO HadoopRDD: Input split: file:/home/ggupta1/project4/examples/e.txt:32+32
24/10/11 11:09:21 INFO HadoopRDD: Input split: file:/home/ggupta1/project4/examples/e.txt:0+32
24/10/11 11:09:21 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1133 bytes result sent to driver
24/10/11 11:09:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1133 bytes result sent to driver
24/10/11 11:09:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (exp-1-31.expanse.sdsc.edu, executor driver, partition 0, PROCESS_LOCAL, 4500 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
24/10/11 11:09:21 INFO HadoopRDD: Input split: file:/home/ggupta1/project4/examples/d.txt:0+5
24/10/11 11:09:21 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 197 ms on exp-1-31.expanse.sdsc.edu (executor driver) (1/2)
24/10/11 11:09:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 220 ms on exp-1-31.expanse.sdsc.edu (executor driver) (2/2)
24/10/11 11:09:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/10/11 11:09:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (exp-1-31.expanse.sdsc.edu, executor driver, partition 1, PROCESS_LOCAL, 4500 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
24/10/11 11:09:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1133 bytes result sent to driver
24/10/11 11:09:21 INFO DAGScheduler: ShuffleMapStage 0 (map at JoinSpark.scala:12) finished in 0.373 s
24/10/11 11:09:21 INFO DAGScheduler: looking for newly runnable stages
24/10/11 11:09:21 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
24/10/11 11:09:21 INFO DAGScheduler: waiting: Set(ResultStage 2)
24/10/11 11:09:21 INFO DAGScheduler: failed: Set()
24/10/11 11:09:21 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (exp-1-31.expanse.sdsc.edu, executor driver, partition 2, PROCESS_LOCAL, 4500 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO HadoopRDD: Input split: file:/home/ggupta1/project4/examples/d.txt:5+5
24/10/11 11:09:21 INFO Executor: Running task 2.0 in stage 1.0 (TID 4)
24/10/11 11:09:21 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1133 bytes result sent to driver
24/10/11 11:09:21 INFO HadoopRDD: Input split: file:/home/ggupta1/project4/examples/d.txt:10+1
24/10/11 11:09:21 INFO Executor: Finished task 2.0 in stage 1.0 (TID 4). 961 bytes result sent to driver
24/10/11 11:09:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 63 ms on exp-1-31.expanse.sdsc.edu (executor driver) (1/3)
24/10/11 11:09:21 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 15 ms on exp-1-31.expanse.sdsc.edu (executor driver) (2/3)
24/10/11 11:09:21 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 35 ms on exp-1-31.expanse.sdsc.edu (executor driver) (3/3)
24/10/11 11:09:21 INFO DAGScheduler: ShuffleMapStage 1 (map at JoinSpark.scala:12) finished in 0.279 s
24/10/11 11:09:21 INFO DAGScheduler: looking for newly runnable stages
24/10/11 11:09:21 INFO DAGScheduler: running: Set()
24/10/11 11:09:21 INFO DAGScheduler: waiting: Set(ResultStage 2)
24/10/11 11:09:21 INFO DAGScheduler: failed: Set()
24/10/11 11:09:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at saveAsTextFile at JoinSpark.scala:14), which has no missing parents
24/10/11 11:09:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/10/11 11:09:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 102.2 KiB, free 433.8 MiB)
24/10/11 11:09:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 433.8 MiB)
24/10/11 11:09:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on exp-1-31.expanse.sdsc.edu:44735 (size: 37.1 KiB, free: 434.3 MiB)
24/10/11 11:09:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
24/10/11 11:09:21 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at saveAsTextFile at JoinSpark.scala:14) (first 15 tasks are for partitions Vector(0, 1, 2))
24/10/11 11:09:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0
24/10/11 11:09:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (exp-1-31.expanse.sdsc.edu, executor driver, partition 0, PROCESS_LOCAL, 4334 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (exp-1-31.expanse.sdsc.edu, executor driver, partition 1, PROCESS_LOCAL, 4334 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
24/10/11 11:09:21 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Getting 2 (456.0 B) non-empty blocks including 2 (456.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms
24/10/11 11:09:21 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/10/11 11:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/10/11 11:09:21 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
24/10/11 11:09:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
24/10/11 11:09:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
24/10/11 11:09:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
24/10/11 11:09:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
24/10/11 11:09:21 INFO FileOutputCommitter: Saved output of task 'attempt_202410111109206155842553404272591_0012_m_000000_0' to file:/home/ggupta1/project4/examples/output/_temporary/0/task_202410111109206155842553404272591_0012_m_000000
24/10/11 11:09:21 INFO SparkHadoopMapRedUtil: attempt_202410111109206155842553404272591_0012_m_000000_0: Committed
24/10/11 11:09:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1545 bytes result sent to driver
24/10/11 11:09:21 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (exp-1-31.expanse.sdsc.edu, executor driver, partition 2, PROCESS_LOCAL, 4334 bytes) taskResourceAssignments Map()
24/10/11 11:09:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 377 ms on exp-1-31.expanse.sdsc.edu (executor driver) (1/3)
24/10/11 11:09:21 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)
24/10/11 11:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/10/11 11:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/10/11 11:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/10/11 11:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/10/11 11:09:22 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
24/10/11 11:09:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
24/10/11 11:09:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
24/10/11 11:09:22 INFO FileOutputCommitter: Saved output of task 'attempt_202410111109206155842553404272591_0012_m_000001_0' to file:/home/ggupta1/project4/examples/output/_temporary/0/task_202410111109206155842553404272591_0012_m_000001
24/10/11 11:09:22 INFO SparkHadoopMapRedUtil: attempt_202410111109206155842553404272591_0012_m_000001_0: Committed
24/10/11 11:09:22 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1588 bytes result sent to driver
24/10/11 11:09:22 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 414 ms on exp-1-31.expanse.sdsc.edu (executor driver) (2/3)
24/10/11 11:09:22 INFO FileOutputCommitter: Saved output of task 'attempt_202410111109206155842553404272591_0012_m_000002_0' to file:/home/ggupta1/project4/examples/output/_temporary/0/task_202410111109206155842553404272591_0012_m_000002
24/10/11 11:09:22 INFO SparkHadoopMapRedUtil: attempt_202410111109206155842553404272591_0012_m_000002_0: Committed
24/10/11 11:09:22 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1588 bytes result sent to driver
24/10/11 11:09:22 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 256 ms on exp-1-31.expanse.sdsc.edu (executor driver) (3/3)
24/10/11 11:09:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/10/11 11:09:22 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.657 s
24/10/11 11:09:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/10/11 11:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/10/11 11:09:22 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 1.133687 s
24/10/11 11:09:22 INFO SparkHadoopWriter: Start to commit write Job job_202410111109206155842553404272591_0012.
24/10/11 11:09:22 INFO SparkHadoopWriter: Write Job job_202410111109206155842553404272591_0012 committed. Elapsed time: 239 ms.
24/10/11 11:09:22 INFO SparkUI: Stopped Spark web UI at http://exp-1-31.expanse.sdsc.edu:4040
24/10/11 11:09:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/10/11 11:09:22 INFO MemoryStore: MemoryStore cleared
24/10/11 11:09:22 INFO BlockManager: BlockManager stopped
24/10/11 11:09:22 INFO BlockManagerMaster: BlockManagerMaster stopped
24/10/11 11:09:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/10/11 11:09:22 INFO SparkContext: Successfully stopped SparkContext
24/10/11 11:09:22 INFO ShutdownHookManager: Shutdown hook called
24/10/11 11:09:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-d43ff00a-9136-4ded-acb3-9d79ce25cee6
24/10/11 11:09:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-c352b939-5f18-4802-8e41-62daaa46bbfb
